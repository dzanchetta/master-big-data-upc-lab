---
jupyter:
  jupytext:
    formats: ipynb,Rmd:rmarkdown, R
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.0'
      jupytext_version: 0.8.5
  kernelspec:
    display_name: R
    language: R
    name: ir
  language_info:
    codemirror_mode: r
    file_extension: .r
    mimetype: text/x-r-source
    name: R
    pygments_lexer: r
    version: 3.5.1
---

```{r}
library(R.utils)
library(keras)  # FashionMNIST dataset
library(nnet)  # Neural networks
```

# Dataset


We're going to use the Kuzushiji dataset which is a dataset that 
contains 60000 training images and 10000 testing images in grayscale (one 
channel) and of size 28x28. Kuzushiji comes in MNIST original format (packed byte-encoded images), so we need a special function to read it.

```{r}
gunzip('kuzushiji/train-images-idx3-ubyte.gz')
gunzip(file.choose())
gunzip('kuzushiji/t10k-images-idx3-ubyte.gz')
gunzip('kuzushiji/train-labels-idx1-ubyte.gz')
gunzip('kuzushiji/t10k-labels-idx1-ubyte.gz')
```

Now we define some auxiliary functions for loading this dataset.
The dataset comes in a binary form. Details about the format can be found on
Yann LeCun's website http://yann.lecun.com/exdb/mnist/

```{r}
load_image_file <- function(filename) {
  ret = list()
  f = file(filename,'rb')
  readBin(f,'integer',n=1,size=4,endian='big')
  ret$n = readBin(f,'integer',n=1,size=4,endian='big')
  nrow = readBin(f,'integer',n=1,size=4,endian='big')
  ncol = readBin(f,'integer',n=1,size=4,endian='big')
  x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
  ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
  close(f)
  ret
}

load_label_file <- function(filename) {
  f = file(filename,'rb')
  readBin(f,'integer',n=1,size=4,endian='big')
  n = readBin(f,'integer',n=1,size=4,endian='big')
  y = readBin(f,'integer',n=n,size=1,signed=F)
  close(f)
  y
}
```

Loading train and test images

```{r}
train <- load_image_file('kuzushiji/train-images-idx3-ubyte')
train <- load_image_file(file.choose())
test <- load_image_file('kuzushiji/t10k-images-idx3-ubyte')
test <- load_image_file(file.choose())
```

Loading labels

```{r}
train$y <- load_label_file('kuzushiji/train-labels-idx1-ubyte')
train$y <- load_label_file(file.choose())
test$y <- load_label_file('kuzushiji/t10k-labels-idx1-ubyte') 
test$y <- load_label_file(file.choose()) 
```

We get the following structure:

- train: Training dataset
    + x: the predictors, 28x28 pixels image in grayscale.
    + y: the response
- test: Testing datset (with x and y)

We can see the images with the following function:

```{r}
str(train)
```

For CNNs we need to have them in 28x28 format instead of an array of 784 pixels

```{r}
dim(train$x) <- c(dim(train$x)[1], 28,28)
```

```{r}
dim(test$x) <- c(dim(test$x)[1], 28,28)
```

## Response reencode


Notice that in y we have an integer from 0 to 9 (10 classes). They are in fact the following:
- 0: お - o
- 1: き - ki 
- 2: す - su 
- 3: つ - tsu
- 4: な - na
- 5: は - ha
- 6: ま - ma
- 7: や - ya
- 8: れ - re 
- 9: を - wo

We recode the response variable to factor.

```{r}
classmap <- read.csv("kuzushiji/kmnist_classmap.csv")
classmap <- read.csv(file.choose())
```

```{r}
classmap$romaji <- c("o","ki","su","tsu","na","ha","ma","ya","re","wo")
classmap
```

We use the romaji codification just to prevent problems with UTF8 characters

```{r}
classString <- classmap$romaji
# y+1 because 0 is the first class and in R we start indexing at 1!
train$yFactor <- as.factor(classString[train$y+1]) 
test$yFactor <- as.factor(classString[test$y+1])
```

For the CNN we use one hot encoding to produce a vector of 10 values per sample, with a one on the class (probablity of belonging to a given class).

```{r}
train$yOneHot <- class.ind(train$yFactor)
test$yOneHot <- class.ind(test$yFactor)
```

*class.ind* reorders the classes alfabetically, therefore we need to revert this order to the original provided. We use *match* over the column names to get a vector of the reorder to match the column names to **classString**.

```{r}
colnames(train$yOneHot)
classString
(m <- match(classString, colnames(test$yOneHot))) 
```

```{r}
train$yOneHot <- train$yOneHot[,m]
test$yOneHot <- test$yOneHot[,m]
```

Now the order is correct

```{r}
colnames(train$yOneHot)
classString
```

```{r}
str(train$yOneHot)
train$yFactor[1:10]
train$yOneHot[1:10,]
```

## Add missing dimension


Convolutional layers will expect the input to have 4 dimensions:
- Sample dimension
- Height dimension
- Width dimension
- Channel dimension

In our case we have only one channel as the image is grayscale. If it's a color image we would have 3 or 4 channels (Red, Green, Blue and Alpha (transparency)). We need to add the missing dimension, however this will not modify the data. 

```{r}
dim(train$x) <- c(dim(train$x),1)
dim(test$x) <- c(dim(test$x),1)
```

## dataset visualization


By the dataset organization, the elements are inverted. Just for display we flip them using `lim = rev(range(0,1))`

```{r}
rotate <- function(x) apply(x, 2, rev)
show_image <- function(imgarray, col=gray(12:1/12), ...) {
  image((matrix(imgarray, nrow=28)), col=col, ylim = rev(range(0,1)), ...)
}
```

```{r}
show_image(train$x[14,,,])
train$yFactor[14] # This ha is katakana! ハ
show_image(train$x[416,,,])
train$yFactor[416]
```

## Create a dataset for nnet


Now we prepare join the X and the Y in a data.frame.

```{r}
nnetData <- data.frame(train$x, class=train$yFactor)
```

```{r}
nnetDataTest <- data.frame(test$x, class=test$yFactor)
```

# Your code should be here

Parte 2) Predicción sobre los datos Kuzushiji utilizando (Dentro de la carpeta code tenéis el código para poder cargarlo):

Multinom con nnet
MLP con Keras
CNN con Keras

Parte 2.1) Multinom con nnet
```{r}
library(nnet)

multinom_kuzushiji <- function(lambda=0, epochs=100)
{
  multinom.model.ku <- multinom (class ~ ., data=nnetData, maxit=epochs, decay=lambda, MaxNWts=8000)
  
  # Training error
  multinom.train.ku <- apply(multinom.model.ku$fitted.values, 1, which.max)-1
  
  (multinom.train_ct.ku <- table(Truth=nnetData$class, Pred=multinom.train.ku))
  cat("\n\nTraining error", (mean(as.character(multinom.train.ku)!= as.character(nnetData$class))*100),"%\n")
  
  # Test error
  multinom.test.ku <- predict(multinom.model.ku, nnetDataTest)
  
  (multinom.test_ct.ku <- table(Truth=nnetDataTest$class, Pred=multinom.test.ku))
  cat("Test error", (mean(as.character(multinom.test.ku) != as.character(nnetDataTest$class))*100),"%\n")
}

#Ha tardado más de 1 hora y no se ha completado. Ha faltado recursos en mi maquina para ejecutarlo.
# Training error 100 %
# Test error 35.14 %
multinom_kuzushiji()
multinom_kuzushiji(lambda = 0.1,epochs = 300)

# Training error 100 %
# Test error 35.34 %
multinom_kuzushiji(lambda = 0.5,epochs = 200)
```
2.1) NNET:
```{r}
library(nnet)
model.nnet <- nnet(class ~., data = nnetData, size=1, maxit=100, decay=0)

#Attempt 1 con 1 hidden layer
#Weigths
model.nnet$wts
#value of fitting criterion plus weight decay term.
model.nnet$value
sqrt(model.nnet$wts %*% model.nnet$wts)
#1 if the maximum number of iterations was reached, otherwise 0.
model.nnet$convergence

#Attempt 2 con 10 hidden layer
model.nnet2 <- nnet(class ~., data = nnetData, size=10, maxit=100, decay=1,MaxNWts=8000)

model.nnet2$value
sqrt(model.nnet2$wts %*% model.nnet2$wts)

#Training error for model.nnet2 --> 35.66% de error en training
pred1 <- as.factor(predict(model.nnet2, type="class"))
cmatrix <- table(Truth=nnetData$class, Pred=pred1)
(1-sum(diag(cmatrix))/sum(cmatrix))*100

#Test error for model.nnet2 (51.37% de error en test!!!)
pred2 <- as.factor(predict(model.nnet2, newdata=nnetDataTest, type="class"))
cmatrix2 <- table(Truth=nnetDataTest$class, Pred=pred2)
(1-sum(diag(cmatrix2))/sum(cmatrix2))*100
```

Parte 2.2) MLP con Keras
```{r}
library(dplyr)
library(keras)

#### Pre-processing antes de ejecutar Keras ####
#Train data and test data without class
nnetData_x <- nnetData[, -785]
nnetDataTest_x <- nnetDataTest[, -785]

levels.class <- length(levels(nnetData$class))
nnetData_y_num <- as.integer(nnetData$class)
nnetDataTest_y_num <- as.integer(nnetDataTest$class)
nnetData_y <- to_categorical(nnetData_y_num-1, levels.class)
nnetDataTest_y <- to_categorical(nnetDataTest_y_num-1, levels.class)

#Predictors transformados a matrix, pues Keras no funciona con data.frame
matrix_train_nnetData <- as.matrix(nnetData_x)
matrix_test_nnetDataTest <- as.matrix(nnetDataTest_x)

#### Keras con 1 layer con Softmax y utilizando como función de activación el ReLU ####

per <- keras_model_sequential()
per %>% layer_dense(units = 128, activation = "relu",
              input_shape = c(ncol(nnetData_x))) %>%
        layer_dense(units = levels.class, activation = "softmax")

# Ahora compilamos con validation metric (loss function = binary_crossentropy) y un optimizador (accuracy)
per %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(), 
  metrics = c("accuracy") #evaluación de lo buena que es la red neuronal
)

per

# Entrenando el modelo de red neuronal con 1 layer --> 93.53% de accuracy en training
start.time <- Sys.time()
history <- per %>% fit(
  matrix_train_nnetData, nnetData_y,
  epochs = 100, batch_size = 128, 
  validation_split = 0.2
)

per_time <- Sys.time() - start.time
print(per_time)

plot(history)
history

#Probar el modelo --> 90.82% de accuracy en test
per_test <- per %>% evaluate(matrix_test_nnetDataTest, nnetDataTest_y)
per_test

#### Keras con 4 layers con Softmax y utilizando como función de activación el ReLU ####

per <- keras_model_sequential()
per %>% layer_dense(units = 128, activation = "relu",
              input_shape = c(ncol(nnetData_x))) %>%
        layer_dense(units = 64, activation = "relu") %>%
        layer_dense(units = 32, activation = "relu") %>%
        layer_dense(units = 16, activation = "relu") %>%
        layer_dense(units = levels.class, activation = "softmax")

# Ahora compilamos con validation metric (loss function = binary_crossentropy) y un optimizador (accuracy)
per %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(), 
  metrics = c("accuracy") #evaluación de lo buena que es la red neuronal
)

per

# Entrenando el modelo de red neuronal --> 93.54%
start.time <- Sys.time()
history <- per %>% fit(
  matrix_train_nnetData, nnetData_y,
  epochs = 200, batch_size = 128,
  validation_split = 0.2
)
per_time <- Sys.time() - start.time
print(per_time)

plot(history)
history

#Probar el modelo --> 92.53% de accuracy
per_test <- per %>% evaluate(matrix_test_nnetDataTest, nnetDataTest_y)
per_test

#### Keras con 4 layers con Softmax y utilizando como función de activación el ReLU + Drop_out para utilizar menos tiempo de red ####

per <- keras_model_sequential()
per %>% layer_dense(units = 128, activation = "relu",
              input_shape = c(ncol(nnetData_x))) %>%
        #layer_dropout(rate = 0.25)
        layer_dense(units = 64, activation = "relu") %>%
        #layer_dropout(rate = 0.15)
        layer_dense(units = 32, activation = "relu") %>%
        #layer_dropout(rate = 0.10)
        layer_dense(units = 16, activation = "relu") %>%
        #layer_dropout(rate = 0.05)
        layer_dense(units = levels.class, activation = "softmax")

# Ahora compilamos con validation metric (loss function = binary_crossentropy) y un optimizador (accuracy)
per %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(), 
  metrics = c("accuracy") #evaluación de lo buena que es la red neuronal
)

per

# Entrenando el modelo de red neuronal
start.time <- Sys.time()
history <- per %>% fit(
  matrix_train_nnetData, nnetData_y,
  epochs = 100, batch_size = 128, #epochs: numero de passadas no dataset
  validation_split = 0.2
)
per_time <- Sys.time() - start.time
print(per_time)

plot(history)
history

#Probar el modelo
per_test <- per %>% evaluate(matrix_test_nnetDataTest, nnetDataTest_y)
per_test
```

2.3) CNN (Convolutional neural network) con Keras
```{r}
library(dplyr)
library(keras)
lenet <- keras_model_sequential() %>%
    layer_conv_2d(filters=20, kernel_size=c(5,5), activation="tanh",
        input_shape=c(28,28,1), padding="same") %>% 
    layer_max_pooling_2d(pool_size=c(2,2),strides=c(2,2)) %>%
    layer_conv_2d(filters=50, kernel_size=c(5,5), activation="tanh",
        input_shape=c(28,28,1), padding="same") %>% 
    layer_max_pooling_2d(pool_size=c(2,2),strides=c(2,2)) %>%
    layer_flatten() %>%
    layer_dense(units=500, activation="tanh") %>%
    layer_dense(units=10, activation="softmax") 

lenet

sgd <- optimizer_sgd(
                     lr=0.05, 
                     decay=0.001, 
                     momentum=0.8, 
                     clipnorm=1.
)
lenet %>% compile(optimizer=sgd,
              loss='categorical_crossentropy', 
              metrics = "accuracy"
)

#Entrenar el modelo lenet --> 99.98% de accuracy en los datos de training
lenet %>% fit(
          train$x, 
          train$yOneHot, 
          batch_size=50,
          validation_split=0.2,
          epochs=10
)

#Grabar el modelo
lenet %>% save_model_hdf5("lenet-kuzushiji.h5")

#Predict el modelo
lenet <- load_model_hdf5("lenet-kuzushiji.h5")

testnumeric <- as.numeric(test$x)
dim(testnumeric) <- dim(test$x)
pred_prob <- predict(lenet, testnumeric)
head(pred_prob)

#For each element we get the probability of that element to be of each class, therefore we search for the value that is maximum in each row and then we create the confusion matrix.

predClass <- apply(pred_prob,1,which.max)
predClass <- classString[predClass] # And change the integers by their class tag


trueClass <- test$yFactor

# Now we do a confusion matrix and analyze it
(cMatrix <- table(trueClass,predClass))

correctClass <- sum(diag(cMatrix))
total <- sum(cMatrix)
(accuracy <- correctClass/total)
#93.9% de accuracy en test
```